{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## REUSING BLUEPRINTS, HYPERPARAMETERS and DEPLOYING MODELS via API\n",
    "\n",
    "**Author**: Tim Whittaker\n",
    "\n",
    "The point of this script is to illustrate the following\n",
    "<a id=\"toc\"></a>\n",
    "1. [Pull blueprint for a model from an existing project](#ebp)\n",
    "2. [Train that blueprint in a new project with a new data set](#tbp)\n",
    "3. [Deploy the model (or replace in a current deployment)](#deploy)\n",
    "4. [Keep the hyper parameters for step 2](#savehp)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements\n",
    "* DataRobot Modeling API\n",
    "Please us `pip install datarobot --upgrade` to get latest and greatest.  \n",
    "\n",
    "__This example assumes that you have built a project using the wine quality dataset, and the project id and a specific model id are available.  If not, see wine_autopilot.py__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# # DataRobot upgrade command below if needed\n",
    "# !pip install datarobot --upgrade"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import datarobot as dr\n",
    "from config import *\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import yaml"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## get data\n",
    "## we are actually going to break this up and use half as new data\n",
    "## the project we are pulling from was built on the entire wine-quality dataset.\n",
    "wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", delimiter=\";\")\n",
    "np.random.seed(1)\n",
    "msk = np.random.rand(len(wine)) < 0.75\n",
    "old_data = wine[msk]\n",
    "new_data = wine[~msk]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "new_data.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1219.000000       1219.000000  1219.000000     1219.000000   \n",
       "mean        6.845406          0.276948     0.336678        6.399754   \n",
       "std         0.861629          0.096943     0.126456        5.078021   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.400000          0.320000     0.390000       10.000000   \n",
       "max        11.800000          1.005000     1.660000       31.600000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1219.000000          1219.000000           1219.000000  1219.000000   \n",
       "mean      0.045771            35.106235            137.548811     0.993951   \n",
       "std       0.022840            16.396451             42.710354     0.003004   \n",
       "min       0.014000             3.000000             18.000000     0.987130   \n",
       "25%       0.035000            23.000000            107.000000     0.991665   \n",
       "50%       0.042000            33.000000            133.000000     0.993700   \n",
       "75%       0.050000            45.000000            167.000000     0.995920   \n",
       "max       0.301000           118.500000            366.500000     1.010300   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1219.000000  1219.000000  1219.000000  1219.000000  \n",
       "mean      3.187916     0.484668    10.564731     5.922888  \n",
       "std       0.149502     0.107361     1.246939     0.902811  \n",
       "min       2.790000     0.230000     8.400000     3.000000  \n",
       "25%       3.090000     0.400000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.540000    11.400000     6.000000  \n",
       "max       3.810000     1.000000    14.000000     9.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "      <td>1219.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.845406</td>\n",
       "      <td>0.276948</td>\n",
       "      <td>0.336678</td>\n",
       "      <td>6.399754</td>\n",
       "      <td>0.045771</td>\n",
       "      <td>35.106235</td>\n",
       "      <td>137.548811</td>\n",
       "      <td>0.993951</td>\n",
       "      <td>3.187916</td>\n",
       "      <td>0.484668</td>\n",
       "      <td>10.564731</td>\n",
       "      <td>5.922888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.861629</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>0.126456</td>\n",
       "      <td>5.078021</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>16.396451</td>\n",
       "      <td>42.710354</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.149502</td>\n",
       "      <td>0.107361</td>\n",
       "      <td>1.246939</td>\n",
       "      <td>0.902811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.987130</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.991665</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.995920</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.800000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>118.500000</td>\n",
       "      <td>366.500000</td>\n",
       "      <td>1.010300</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"ebp\"></a>\n",
    "## Get an existing blueprint\n",
    "[Table of Contents](#toc) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "## config datarobot Client\n",
    "## Don't keep api token in script.  Place your token in a config file\n",
    "## so there is no concern of accidentally sharing.  See config.py\n",
    "dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n",
    "\n",
    "## original project id and original model id are accessible from the gui url\n",
    "## just click on desired model in leaderboard and pull approapriate ids from url\n",
    "## for example.\n",
    "## https://app.datarobot.com/projects/<project_id>/models/<model_id>/blueprint\n",
    "original_pid = \"5cf71ab5d9436e2c4d0c7a7b\"\n",
    "original_mid = \"5cf71c005ff3772856c2a81b\"\n",
    "\n",
    "## ============================================================================#\n",
    "## 1. Pull blueprint for a model from an existing project\n",
    "## all we now at this point is the original project id\n",
    "## as well as the model id we want to use.\n",
    "## the project id and model id are available in gui by clicking on the model\n",
    "## and pulling info from url, for example\n",
    "## https://app.datarobot.com/projects/<project_id>/models/<model_id>/blueprint\n",
    "\n",
    "re_orig_project = dr.Project.get(project_id=original_pid)\n",
    "blueprints = re_orig_project.get_blueprints()\n",
    "models = re_orig_project.get_models()\n",
    "\n",
    "xgb_model = [m for m in models if m.id == original_mid].pop()\n",
    "\n",
    "xgb_blueprint = [bp for bp in blueprints if bp.id == xgb_model.blueprint_id].pop()\n",
    "## instead of finding the particular blueprint, we could just use\n",
    "# xgb_blueprint = xgb_model.blueprint_id\n",
    "## be advised this returns a string and not an actual Blueprint object.\n",
    "## ============================================================================#\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"tbp\"></a>\n",
    "## Train that blueprint in a new project with a new data set\n",
    "[Table of Contents](#toc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## ============================================================================#\n",
    "## 2. Train on that blueprint in a new project with a new data set\n",
    "## one thing to consider - do we want the same exact set of hyperparameters and blueprint\n",
    "## used in the previous project (case a), or do we just want the same blue print (case b)\n",
    "## and let DataRobot figure out the new set of best hyperparameters for the data?\n",
    "## there is a chance it will learn the same hyperparameters on the new data.  \n",
    "\n",
    "## create a new project\n",
    "new_project = dr.Project.create(sourcedata=new_data,\n",
    "                           project_name='new wine data {}'.format(datetime.now()))\n",
    "new_project.set_target(target=\"quality\", mode=\"manual\")\n",
    "\n",
    "## here we are using the blueprint only.\n",
    "## as DataRobot runs the model is will select the best hyperparameters based on the\n",
    "## data.  It is entirely possible that DataRobot will select the same hyperparameters \n",
    "## as in the original project.  \n",
    "new_project.train(xgb_blueprint, source_project_id=original_pid, sample_pct=64)\n",
    "## the following would also have worked\n",
    "# new_project.train(xgb_model.blueprint_id, source_project_id=original_pid, sample_pct=43)\n",
    "model_job = new_project.get_model_jobs()\n",
    "done = model_job[0].get_result_when_complete()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "new_features = new_project.get_features()\n",
    "orig_features = re_orig_project.get_features()\n",
    "\n",
    "new_features = set( [ (f.name, f.feature_type) for f in new_features])\n",
    "orig_features = set( [ (f.name, f.feature_type) for f in orig_features])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "if orig_features.difference(new_features) != set():\n",
    "    print(\"features in new project are different from old project on basis of name and type\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reuse hyperparameters from original project"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## now suppose that I wanted to use the same exact set of hyperparameters as used\n",
    "## in the original project.\n",
    "hyper_params = xgb_model.get_advanced_tuning_parameters()\n",
    "## PLEASE BE ADVISED: as of DR Python API 2.17, the \"default_value\" key contains\n",
    "## best of searched parameters.  This may change in a later version.\n",
    "## PLEASE BE ADVISED: some models aren't tunable, thus an exception will be tossed.\n",
    "## If `get_advanced_tuning_parameters` tosses an exception with a 500 internal\n",
    "## server error message, please reach out to support.\n",
    "\n",
    "## the best of searched hyperparameters.\n",
    "best_hyper_params = dict([(param[\"parameter_id\"], param[\"default_value\"]) for param in hyper_params[\"tuning_parameters\"]])\n",
    "new_xgb = new_project.get_models()[0]\n",
    "model_job = new_xgb.advanced_tune(best_hyper_params, description=\"hyperparameters from original project {}\".format(original_pid))\n",
    "## PLEASE BE ADVISED: an exception will be tossed if a model on the leaderboard with sameblueprint has the\n",
    "## same set of hyperparameters.  This means that when we trainined on the blueprint, and let DR learn they \n",
    "## hyperparameters, it learned the same set.  \n",
    "new_xgb_tuned = model_job.get_result_when_complete()\n",
    "\n",
    "## next open the leaderboard browser to view the models.\n",
    "## the model with the old hyperparameters will have a description as set above.\n",
    "new_project.open_leaderboard_browser()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## Deploy\n",
    "[Table of Contents](#toc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "## 3. Deploy the model (or replace in a current deployment)\n",
    "## model deployment is available in python api 2.17.0 and this script will be\n",
    "## updated soon.\n",
    "print(\"=\")\n",
    "prediction_server = dr.PredictionServer.list()[0]\n",
    "prediction_server.id\n",
    "\n",
    "## grab current deployments\n",
    "deployments = dr.Deployment.list()\n",
    "\n",
    "## let's deploy the xgboost from the original project\n",
    "deployment = dr.Deployment.create_from_learning_model(\n",
    "    xgb_model.id, label='xgBoost Model', description='A new deployment',\n",
    "    default_prediction_server_id=prediction_server.id)\n",
    "\n",
    "print(deployment.id)  ## this is also available via gui url\n",
    "deployment_id = deployment.id\n",
    "## clean up\n",
    "del deployment, deployments"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=\n",
      "5cf913b787cf0a073b663311\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replace Deployment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "## oops, we should have deployed the new xgBoost model tuned with the\n",
    "## original models hyperparameters\n",
    "from datarobot.enums import MODEL_REPLACEMENT_REASON\n",
    "\n",
    "deployment = dr.Deployment.get(deployment_id=deployment_id)\n",
    "\n",
    "print(\"current deployment details\\n\\tmodel type:{}\\n\\tmodel id:{}\".format(deployment.model['type'],deployment.model['id']))\n",
    "\n",
    "deployment.replace_model(new_xgb_tuned.id, MODEL_REPLACEMENT_REASON.OTHER)\n",
    "\n",
    "print(\"new deployment details\\n\\tmodel type:{}\\n\\tmodel id:{}\".format(deployment.model['type'],deployment.model['id']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "current deployment details\n",
      "\tmodel type:eXtreme Gradient Boosted Trees Regressor\n",
      "\tmodel id:5cf71c005ff3772856c2a81b\n",
      "new deployment details\n",
      "\tmodel type:eXtreme Gradient Boosted Trees Regressor (Least-Squares Loss)\n",
      "\tmodel id:5cf91396d9436e76200c7ace\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example of the new model on the leaderboard as well as a description.\n",
    "<img src=\"img/scree-grab.png\"></img>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"savehp\"></a>\n",
    "## Stash hyperparameters\n",
    "[Table of Contents](#toc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "try:\n",
    "    assert(yaml.__version__ == \"5.1\")\n",
    "except:\n",
    "    print(\"loading hyperparameters from yaml may throw and exception.  Try setting Loader=None\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "## 4. Keep the hyper parameters for step 2\n",
    "## In any event, regardless of which model we want to keep we can easily store hyperparameters on disk\n",
    "## options include yaml, pickle, etc.  Yaml example below.\n",
    "\n",
    "with open(\"model_hyperparameters.yaml\", \"w\") as f:\n",
    "    f.write( yaml.dump(hyper_params))\n",
    "\n",
    "## load hyperparameters back into python dictionary.\n",
    "with open(\"model_hyperparameters.yaml\", \"r\") as f:\n",
    "    hyperparams_dict = yaml.load(f, Loader=yaml.FullLoader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}