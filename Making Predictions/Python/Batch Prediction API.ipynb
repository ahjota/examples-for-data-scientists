{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "\n",
    "The scope of this notebook is to provide instructions on how to use DataRobot's Batch Prediction API to get predictions out of a DataRobot deployed model\n",
    "\n",
    "### Background\n",
    "\n",
    "The Batch Prediction API provides flexible options for intake and output when scoring large datasets using the prediction servers you have already deployed. The API is exposed through the DataRobot Public API and can be consumed using a REST-enabled client or the DataRobot Python Public API bindings.\n",
    "\n",
    "The main features of the API include:\n",
    "\n",
    "- Flexible options for intake and output.\n",
    "- Support for streaming local files and the ability to start scoring while still uploading—while simultaneously downloading the results.\n",
    "- Ability to score large datasets from, and to, Amazon S3 buckets.\n",
    "- Connection to external data sources using JDBC with bidirectional streaming of scoring data and results.\n",
    "- A mix of intake and output options; for example, scoring from a local file to an S3 target.\n",
    "- Protection against prediction server overload with a concurrency control level option.\n",
    "- Inclusion of Prediction Explanations (with an option to add thresholds).\n",
    "- Support for passthrough columns to correlate scored data with source data.\n",
    "- Addition of prediction warnings in the output.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Python version 3.7.3\n",
    "-  DataRobot API version 2.26.0. \n",
    "\n",
    "Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\n",
    "\n",
    "Full documentation of the Python package can be found here: https://docs.datarobot.com/en/docs/predictions/batch/batch-prediction-api/index.html\n",
    "\n",
    "It is assumed you already have a DataRobot <code>Deployment</code> object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Connecting to DataRobot\n",
    "\n",
    "To inititate scoring jobs through the Batch Prediction API, you need two things:\n",
    "\n",
    "- Connect to DataRobot through the `datarobot.Client` command\n",
    "- Have your `DEPLOYMENT_ID` string. Easiest way to find that is to just go through the User Interface and Copy the ID from the URL. For example in the below example, everything after `deployments/` is the ID of the deployment: `https://app.eu.datarobot.com/deployments/232315iijdfsafw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "\n",
    "dr.Client(endpoint='YOUR_ENDPOINT/api/v2', token='YOUR_TOKEN')\n",
    "deployment_id = \"YOUR_DEPLOYMENT_ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Confirming Ingestion and Output \n",
    "\n",
    "DataRobot's Batch Prediction API allows you to score data from and to multiple sources. You should take advantage of the `credentials` and `data sources` you have already established previously through the UI for easy scoring. `Credentials` are basically usernames and passwords while `data sources` are the database that you have previously established a connection, like snowflake.\n",
    "\n",
    "Below is some example code on how to query the `credentials` and `data sources`.\n",
    "\n",
    "\n",
    "Full list of [input options](https://docs.datarobot.com/en/docs/predictions/batch/batch-prediction-api/intake-options.html)\n",
    "\n",
    "Full list of [output options](https://docs.datarobot.com/en/docs/predictions/batch/batch-prediction-api/output-options.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Credential('6064670wqdww', 'DATAROBOT', 'basic'),\n",
       " Credential('606c17dfdwwdwwd', 'github-application-oauth', 'oauth'),\n",
       " Credential('607efadbaddwwdwwd', 'TheoPetropoulos', 's3'),\n",
       " Credential('6156f50adwdwdwdwdwdw', 'SnowflakeCredentials', 'basic')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all credentials\n",
    "dr.Credential.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above example, you can see that I have quite a few credentials. I have my `GitHub` Credentials, some `SnowflakeCredentials` and `s3 credentials. The alphanumerics on the left is just the ID of the credential. I can use that ID to access the credentials through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60646dsddsdsaffa\n"
     ]
    }
   ],
   "source": [
    "# List all datastores\n",
    "dr.DataStore.list()\n",
    "print(dr.DataStore.list()[0].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above example, you can see a list of all the datastores (I only have a snowflake connection), and with a little bit of manipulation, I can also access the ID of each datastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show some examples on how to use the Batch Prediction API Script. The `intake_settings` and `output_settings` can change to your needs. This means that you can *mix and match* as much as you want to to get to the outcome you prefer. Syntax only needs to change to one part of the equation to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring from CSV to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring without Prediction Explanations\n",
    "dr.BatchPredictionJob.score(\n",
    "    deployment_id,\n",
    "    intake_settings={\n",
    "        'type': 'localFile',\n",
    "        'file': 'inputfile.csv' #Path or Pandas or file-like object\n",
    "    },\n",
    "    output_settings={\n",
    "        'type': 'localFile',\n",
    "        'file': 'outputfile.csv'\n",
    "    }\n",
    ")\n",
    "\n",
    "#Scoring With Prediction Explanations\n",
    "dr.BatchPredictionJob.score(\n",
    "    deployment_id,\n",
    "    intake_settings={\n",
    "        'type': 'localFile',\n",
    "        'file': 'inputfile.csv' #Path or Pandas or file-like object\n",
    "    },\n",
    "    output_settings={\n",
    "        'type': 'localFile',\n",
    "        'file': 'outputfile.csv'\n",
    "    },\n",
    "    \n",
    "    max_explanations=3 #Compute prediction explanations for this amount of features\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring from S3 to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.BatchPredictionJob.score(\n",
    "    deployment_id,\n",
    "    intake_settings={\n",
    "        'type': 's3',\n",
    "        'url': 's3://theos-test-bucket/lending_club_scoring.csv',\n",
    "        'credential_id': 'YOUR_CREDENTIAL_ID_FROM_ABOVE',\n",
    "    },\n",
    "    output_settings={\n",
    "        'type': 's3',\n",
    "        'url': 's3://theos-test-bucket/lending_club_scored2.csv',\n",
    "        'credential_id': 'YOUR_CREDENTIAL_ID_FROM_ABOVE'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring from JDBC to JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.BatchPredictionJob.score(\n",
    "    deployment_id,\n",
    "    \n",
    "    intake_settings = {\n",
    "    'type': 'jdbc',\n",
    "    'table': 'table_name',\n",
    "    'schema': 'public',\n",
    "    'dataStoreId': data_store.id, #Put the Id of the datastore you want\n",
    "    'credentialId': cred.credential_id #put the credentials you want\n",
    "    },\n",
    "    \n",
    "    output_settings = {\n",
    "        'type': 'jdbc',\n",
    "        'table': 'table_name',\n",
    "        'schema': 'public',\n",
    "        'statementType': 'insert',\n",
    "        'dataStoreId': data_store.id,\n",
    "        'credentialId': cred.credential_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
